---
title: "mlp1_checkpoint_in_r"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Replicating code

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(keras)
library(tensorflow)
library(tidyverse)
library(dplyr)
library(caret)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
n_units <- 400
layers <- 4
n_batch <- 1024
n_epochs <- 200
```

```{r, echo=FALSE, message=FALSE, warning=FALS}
set.seed(42) # equivalent to python's random_state parameter
train_index <- createDataPartition(y=undef, p = 0.99, list = FALSE)
train_index2 <- createDataPartition(y=undef2, p = 0.99, list = FALSE)

call_x_train <- undef[train_index, ]
call_x_test <- undef[-train_index, ]

call_y_train <- undef2[train_index2, ]
call_y_test <- undef2[-train_index2, ]
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
model1 <- keras_model_sequential()
model1 %>% layer_dense(units = n_units, input_shape = c(7)) %>% layer_activation_leaky_relu()

for (i in 1:(layers-1)){
  model1 <- model1 %>% layer_dense(units = n_units)
  model1 <- model1 %>% layer_activation_leaky_relu()
}

model1 %>% layer_dense(units = 1, activation = 'relu')

compile(object = model1, optimizer = optimizer_adam(lr = 0.0001), loss = 'mse')

summary(model1)
```